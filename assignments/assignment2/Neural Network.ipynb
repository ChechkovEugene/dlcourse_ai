{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 45.861270, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.737422, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.789932, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 46.073907, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.847318, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.517241, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.143701, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.705971, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.901959, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.887821, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.013442, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.129141, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.149481, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.986460, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.300729, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.757450, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.954830, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.998303, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 45.816772, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 45.897925, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11515c8d0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5Bk91XY8e/p1/Rjpntmumd3ZrWyJRmRWICwzUqAjQXGYCRCSZiSQMIkFlClEFAlVAKJqkgEEUVVwIFQJEpikbh4FLZsCA+FkktSjANJsI3WsiR7Lctay/LueGd2Z3pmumf6MT3dffLHvT3bOzuPO9Ove6/Op2pq+nH73t/23nvm9zv39xBVxRhjTHhFRl0AY4wxg2WB3hhjQs4CvTHGhJwFemOMCTkL9MYYE3KxURdgp0KhoNddd92oi2GMMYHy2c9+dllVZ3Z7z3eB/rrrruP06dOjLoYxxgSKiHxtr/csdWOMMSFngd4YY0LOAr0xxoScBXpjjAk5C/TGGBNyFuiNMSbkLNAbY0zI+a4f/ZE1KvB/f7u3fURi8G33w8TxvhTp0J7/MKx8dTTHNiYMbnwPXHvLaI597jNw9n/1to/sCTj1k/0pT5fwBPqtGvzNB3rYgTsvfzwF7/infSnSoWxuwJ//E/eJDP/4xgSewrlPwf1/OZrDf+Lfwtf+Hz1dvydPWaDfV6YAv7J29M+rwq/NQmWpf2U6jM5x7/rP8Nb3jaYMxgTZ4++D4ldGd/yNS/BN74V7fm90ZdiD5eg7RCCdh2pxNMfvHDedH83xjQm6dB6qy6M7fnXZt9evBfpu6TxURnSidI6bKYzm+MYEXaYA1RVot4d/7FYTaquQ9uf1a4G+W6ZgNXpjgipdAG1BvYcU7lHVVp3fPq2oWaDvli6MrulXtRq9MT3pXDujqKx1rl+fVtQs0HdL56Eyohp9ZRmiCUiMj+b4xgRdetr5PYr0a8UCfXBk8tBYh+bm8I9dLTotCrGulcYcSSc/PopWuc9b5Bbou6VH2fQrOn9ojDFHM9LUTecemwV6/+ucKKNq+vn0JDEmEDppk5Fcv51APz38Y3tggb5b50QZVdPPp/k9YwIhnoJ4ZnQ3Y5M5iMaHf2wPLNB369SoR3FDtlL0bX7PmMDIjGgsjM9b5Bbou40qx9fcdG4C+/hEMSYQ0iMaC1P1d0XNAn235CRIZPipm86JaTdjjelNZkRjYTq95nzKAn23SARS08Nv+vm8D64xgTGqsTCVZd/eiAWPgV5EbheRl0XkrIg8tMv7/1xEvigiL4rIJ0TkjV3vvV9EXnF/3t/Pwg/EKGoE26Pq/FsjMCYQOhObqQ7vmKrBT92ISBR4FLgDuAm4T0Ru2rHZ54BTqnoz8CfAb7ifnQZ+Gfh24Fbgl0Vkqn/FH4C0OzHSMHWO5+MTxZhAyBSgWYet6vCOuVmG9pavK2peavS3AmdV9VVVbQCPA3d1b6Cqn1TVzjf7aeCk+/gHgGdUdUVVV4FngNv7U/QBGcVd+4rV6I3pi/QIxsIEYOZZL4H+GuB81/N597W9/DTw8cN8VkQeEJHTInJ6aWlEC390jGJO6+oyIJCaHO5xjQmbUYyFCcDMs14C/W6Tr+yaABORnwBOAZ01/Tx9VlUfU9VTqnpqZmbGQ5EGqJO6abeGd8zOjZxIdHjHNCaMMiMYCxOAzhReAv08cG3X85PAhZ0bicj3Ab8E3Kmqm4f5rK9kCoBCbYhzWvu8a5YxgbFdox9ioN/uHu3fa9hLoH8WuFFErheRBHAv8ET3BiLyVuCDOEH+UtdbTwHvEZEp9ybse9zX/GtUTT8fnyTGBMb2oMdhXr/+v8d2YKBX1SbwIE6Afgn4mKqeEZFHROROd7MPAOPAH4vI8yLyhPvZFeBXcf5YPAs84r7mX6OYGMnnfXCNCYyxLETiw79+YylIpId3zEOKedlIVZ8Entzx2sNdj79vn89+CPjQUQs4dKOqEaTfPrzjGRNWIsPvUBGAFrmNjN1p2HPSt1vOepM+P1GMCYzMkMfCVIu+vhELFuivtr0c2ZACfW0NtO3r/J4xgZIe8liYiv+nGLdAv1NszMnzDavp5/NFhY0JnKGnbpZ93yL3lKMPgspmk//wzJd72kc0Ktz/9uuYG2aNoGvmyj/81Gt8rTjEodvGhMy733yc78wMeariitM9+m/PLvNXX7p08Pb7ODGZ4qe+6/o+Feyy0AT6zWabj/zduZ72UWm0yCbj/NwwTxT3D8p6ZJJ/8xdnSEQjxKO2QLgxh1XbavHC/Bp//PcLUC9Ba2vwKz5t1WCrApk8H3j6ZV6cL5GMHT1RcvPJSQv0+5nOJDjzSG/T6LzlkadZKNWcpl/5630q2QHcJubFVgaA3/qxb+WHbj4xnGMbEyI///jnOP211cvrOlSLMDE72IN2TX+wsFbnvW+9hn9/z7cO9phHYDn6LrPZJIulunNjdFg3Y93jzDecPrhzueRwjmtMyMzmUlws12mnhjgWxj1GK5nn0nrdt9evBfouc7kkC6W6UyMY1pzW1SIkJljYcI41m0sN/pjGhNBcLslWSylHcs4Lw0i/ui3yVcnSVpi1QO9/c5MpJ9CnC9BqQGNj8AetLkMmz8JajYjAsYmxwR/TmBDq1KYvtsadF4bR88btr7/YdI55wqcVNQv0XeaySVYqDbbG3LVRhtX0SxdYKNWZmRgjHrX/EmOOYs4Nshcazv2uoaRf3RhxwU29Wo0+ADr/SatknReG1fRL51ks1y1tY0wPOtfv+ZrbKh5KjX4ZJMq5qtO7x3L0AXBi0gm0F9sTzgvDqNFXVyDj1OhP+PQkMSYI8pkEiWiEC+tNSE0Np6JW6VTUGqTiUXKpAXfnPCIL9F06NYJOM2zgJ4oqVJbRtJOj92uzz5ggiESE47kxt4t0YUgVteJ2RW0ul0TEn2NgLNB3mc06gfZc3U2hDLrp19iA1iabY1NUGi3fNvuMCYq5rNuhYliDHt0JzRZK/q6oWaDvkhmLkU3GOL8RgejY4GsE7v7X3HsClqM3pjezuc5YmCFNY9JJ3ZTqFuiD5MRkioXy5nBqBG7XrCX3noDl6I3pzdykE+g1nR9aZ4p2usDF9U3fdq0EC/RXmc0lL0+DMPBA79Q4FrYy28c2xhzdXDZJo9WmFndvxrbbgztYqwm1NaqxSVpt9fX1a4F+h7lhNv3c/c9vphGBYxP+PVGMCYJO+rMkWdAW1NcGd7DaKqCs4rTI/XyPzQL9DrPZFMsbDVqpIcxp7e7/tWqKwvgYiR5mvTPGXA62y+p2kR5kq9y9fpfd1KvV6ANkbtL5z6rEJgc/sq5ahGiCr66L5eeN6YPO9Xux6Y6OHWSgd1vkCz6f/gAs0F+lUyMoSQ4a69DcHNzB3AULFsubvq4NGBMUhcwYsYgwvz0NwgBb5e4fkfnNFGOxCJNpfw6WAo+BXkRuF5GXReSsiDy0y/u3ichzItIUkbt3vPfrIvIF9+fH+lXwQekE+qJ2JkYacNMv43TNmvNxbcCYoIhEhOPZJF+rdQY9DjLQu6nXWsrXg6XAQ6AXkSjwKHAHcBNwn4jctGOzc8D9wId3fPYfAG8D3gJ8O/CLIpLtvdiD07mZsz0D3iBrBJVlmslp1jebVqM3pk9OTCY5W0k4TwZ6/TqVwK9sjPn++vVSo78VOKuqr6pqA3gcuKt7A1V9TVVfBHb2ZboJ+GtVbapqBXgB6G0ZqAEbH4sxkYwxvzmMGkGRWmwS8Pcde2OCZDaX4lxZIZ7ZHqsyENVlSOaYLzd9nZ8Hb4H+GuB81/N59zUvXgDuEJG0iBSAdwHX7txIRB4QkdMicnppacnjrgdnLtfd9BvkiVKkHOkEen+fKMYERWcBIc0MuOdctYimC1ws+3tULHgL9LslnjwtvaSqTwNPAn8LfAT4FNDcZbvHVPWUqp6amZnxsuuBms2l+ErVnep0UE2/5iZslllxpz+wGr0x/TGbTdJotmklBzwWprJMc2yKZlt9f/16CfTzXFkLPwlc8HoAVf01VX2Lqn4/zh+NVw5XxOGbyyZ5pRwHiQyuRuDe5F1yFwU/lrWVpYzphxNuF8tafHLgNfpq3FmkyO/zVHkJ9M8CN4rI9SKSAO4FnvCycxGJikjefXwzcDPw9FELOyxzk0mWKltoanpwNQI30F/YylAYH2MsFh3McYx5nekE3fVIbrCp18ry9vq0ga/Rq2oTeBB4CngJ+JiqnhGRR0TkTgARuUVE5oF7gA+KyBn343Hg/4jIF4HHgJ9w9+drc7kkqtBMTg+ue6X7B+RcPe37k8SYIOlcT6uSHVxFTRWqxe3V6Px+Dce8bKSqT+Lk2rtfe7jr8bM4KZ2dn6vj9LwJlE6NoBafJD6oQO/u99Vqitlj/j5JjAmSwrgzaGq5PQHNGjQqkMj09yCbZWhvsdTKkIhFmM4k+rv/PrORsbvoTEewEZ0cXI3A3e8rGwmb/sCYPoq6g6YWtwY4OrazKPjWuO8HS4EF+l1dsUj4AG/GKsL5etL3N3KMCZrZXJLzmwNcEtTd57nN1PbKdH5mgX4XE8k442Mxp+lXW4V2q/8HqS7TTk7RJuL7/J4xQTObS/LVWmdJ0MEF+lerqUBcvxbo9zCbc5t+2obaAOa0riyzmZjePpYxpn9O5JK8sjHAsTDuPp3pD/zfIrdAv4e5XJLzjQFOg1AtUok5XbP8PnzamKCZzaUu5+gHcv06+7zYGt/ut+9nFuj3MJdL8tWq+x84iBpBteisgoMNljKm3+ZyScqk0Uh8MKmbyjLtaJIaScvRB9lsLsVXa+5/4IBOlKJmyWcSJOM2WMqYfnLSoUIjMTWgitoKmwlnVGwQ5qmyQL+HuVySYtudUbnfTb92G2orXGyNW37emAHopEOr8ckB3YxdpurOPBuEa9gC/R7mcsntRX/7vqRgfQ20zdcb6UDUBowJmpmJMaIRoTyo0bGVZUqRHIlohLzPB0uBBfo9zeVSNIizFRvvf43ePfG+VgtG1yxjgiYaEY5NjDmzww6oRl/UCY7nxohE/D1YCizQ76nTHKvFBtD0c/9wzDcygWj2GRNEs7kkl1oDqKgBVFe41BpnLhuMFrkF+j1kkzHSiSjlaK7/TT93fys6YTV6YwbkRC7Fha0M1EvQ2urfjrfq0Njg6410YCpqFuj3ICLM5ZKs6ET/awRuC8EJ9MGoERgTNLO5JOfrndGxfZyu2I0H52pp5gLQhx4s0O9rLpfiUnui/zdj3RNlhazV6I0ZkLlcksXmuPOkn5U1t0V+qT3OXAD60IMF+n3N5pIsNjJODVw9rZ7oTaVII5qhQTwwTT9jgmYul9peqrOv6deuFnkQpj8AC/T7OpFLcr6RgtYmNDb6t+PqMuvRHNM2WMqYgZnNJSnqAMbCdAI92UBMfwAW6Pc1m0tdPlH6XCMokQ3E0Gljgmoul2RV3bEw/czRu7GgqNnAtMgt0O9j7ooaQR/z9JVllqzHjTEDdWxijJK4Ofq+VtSWaROlGslQyARjnioL9PuYvaJG0MdAXy2yuGV96I0ZpFg0Qn4iQyXa5wWEqkUq0SzHsulADJYCC/T7OpFLUdyeBqFPJ4oqWllmoTnOiclg3MgxJqhmc0lnlth+1ugry6xJcPLzYIF+X9lUjGrMmaGubzWCRgVpbbKqE5ajN2bAnPTrRN9b5Mvt4PS4AY+BXkRuF5GXReSsiDy0y/u3ichzItIUkbt3vPcbInJGRF4Skd8Rv6+i20VEyGUn2ZJ4/2oE233oLUdvzKDN5pJcbI6jfQz0nRZ5kK7fAwO9iESBR4E7gJuA+0Tkph2bnQPuBz6847NvB94B3Ax8M3AL8N09l3qIZidTzgx4/bpr7w6+CtIde2OC6kQuxaXWONrH1I1WixTb44FqkXup0d8KnFXVV1W1ATwO3NW9gaq+pqovAu0dn1UgCSSAMSAOXOy51EM01+li2a/UjbufVZv+wJiBm3WnG5dq0VkHolftFlJbDVQfevAW6K8Bznc9n3dfO5Cqfgr4JLDg/jylqi/t3E5EHhCR0yJyemlpycuuh2Yul+RiP2sEbhNyKzlNKmGDpYwZJGe+qiyiLdgs9b7D6gqCUgzQqFjwFuh3y6l7mg9ARL4BeDNwEuePw/eKyG1X7Uz1MVU9paqnZmZmvOx6aGbdmzmtjT79AXL/YCSyx/qzP2PMnuYmuwc99iFP37nHpsGap8pLoJ8Hru16fhK44HH/7wU+raobqroBfBz4jsMVcbQ6o+ukXzn66jJbxMjlpvqzP2PMno5NjF1eKa4f6Ve3RV6SLIXxYAyWAm+B/lngRhG5XkQSwL3AEx73fw74bhGJiUgc50bsVakbP+vk6KNb69Dc7H2HlSKrZJmbSve+L2PMvuLRCJrOO0/6kX519yGZAtGADJYCD4FeVZvAg8BTOEH6Y6p6RkQeEZE7AUTkFhGZB+4BPigiZ9yP/wnwFeDzwAvAC6r6Pwfw7xiYuVzy8gx4feii1aoss9yeCMz0psYEXXzCTQf3o4ul2yqIByz1GvOykao+CTy547WHux4/i5PS2fm5FvCPeyzjSE2m45QjOedJZRmyJ3raX3N9yb2RY4HemGFIT87CKv1J3bh5/vRUsAK9jYw9gIgQybhNvz7UCLSy7C44Epw79sYEWX4qR1XH+nIzVqvLlDXN8dx4H0o2PBboPYhPuH+9+xDoo7Wis4RggPrgGhNkTvp1gq31Sz3va6vstMjnAjZPlQV6D9KTbqDv9WZOs0G8ueGsTGM5emOGorMASaPcexfpxvpSIJcAtUDvQXZ6hpYK7UqPJ4rbIqjGJ8mMebo9Yozp0YnJFCs6QbsPvW60ssRKAKcvsUDvwezUOGuMUy/1GujdEy1d6L1QxhhPZrNOz7lIrR+p11VWdIITAbvHZoHeg7msM4y6Ue4xx+fWKGITFuiNGZbj2SQrOkFis8dBj6qMNVZYlSwzE8EZLAUW6D2ZdW/mtHudBsFN3SRzweqaZUyQJWIR6okp4u1NaFSPvqPNMlFt0khMBWqwFFig96Szdmyk1luNYMv9Q5GZmutHsYwxXnVGx/bSl95tkWsAU68W6D2YziQoSZb45mpP+6muXqStwlTBavTGDFMk446O7eWGrDvfVTyAqVcL9B6ICI2xKVLNUk9zWtfXLrJGhtnJTB9LZ4w5yFi292kQ1O11lwhg6tUCvUftVJ4IbagdvVbf3FgO3PSmxoRBZnoWgHrp6B0qamvOZ8enZvtSpmGyQO9RNOM213rI8Um1SJFsoBYsMCYMsnknOG+sLB55HxurzmdzheN9KdMwWaD3aMxtrrU3jh7oY/UVypEs4zZYypihKuRn2NLodq38KOqlS9Q1zrHpfB9LNhwW6D1KTzl/xddXj14jSDZWaSRswRFjhm1uMs1qj/PdNNeXKJIN3Dw3YIHes2ze6RK5XjxioG+3ybTLNJPBqw0YE3THc2MUdQLtZWLCijMhYdAGS4EFes/yM06gr61dPNoO6mtEaSMB7INrTNCNxaKsR3NEexgLE9ssUonmiEeDFzaDV+IRmc3nWNcUW+tHy9F3pk+IZ/21+Lkxrxf1+BTJxtEDfbKxRj0x3ccSDY8Feo+m0wlWmdjuS3tYq8sLAKQmg9cH15gwaI5Nk2mVjvz58dYaraQF+lCLRIT16OSRm35lN7c/MR28PrjGhEKmwIRuQGvr0B/VrRpp6pAJZurVAv0h1OOTjDWONmCquurk9qdnbJ4bY0Yh6k5dUD3CoKkN9/pNTAQz9WqB/hCayWnSrbUjfXaz5JwoheO9LS5ujDmaZM7pIl28dOHQny1eclKvyYCmXj0FehG5XUReFpGzIvLQLu/fJiLPiUhTRO7uev1dIvJ8109dRH64n/+AYdJ0nsl2mXbr8PPdtCpFKppkYnxiACUzxhxkYtoJ9KXlw3eRLi1fcPcRzNTrgYFeRKLAo8AdwE3AfSJy047NzgH3Ax/uflFVP6mqb1HVtwDfC1SBp/tQ7pGIjs8wJluslg6fvolUi5QjuQGUyhjjxWTBSZt20jCH0Um9TgY09eqlRn8rcFZVX1XVBvA4cFf3Bqr6mqq+COxX1b0b+Liq9jDz/2h1FgxZvvj1Q382sblCNT7Z7yIZYzyannHSpp006mFsut2j8zPBTL16CfTXAOe7ns+7rx3WvcBHdntDRB4QkdMicnppqfeV2gdl3G22rR6h6ZfaWqUxFsyuWcaEQdIdw9JcP3yMaW0s0yRCPBPMa9hLoN9tzSw9zEFEZA74FuCp3d5X1cdU9ZSqnpqZ8e9d7VzBCfSVQ853s9VqM6Fl2qlgniTGhEI0xrqMby8gchhSLbIhWYgEs/+Kl1LPA9d2PT8JHPa29Y8Cf6aqh+/A6iOTeafZVj/kDHiXynXylC+vcmOMGYlKbJJY/fCBPrG5QiUW3NSrl0D/LHCjiFwvIgmcFMwThzzOfeyRtgmSSMaZkKx5yKmKLxWLJGWLhE1/YMxIbSamSG0dPtCntkpsjQV35tkDA72qNoEHcdIuLwEfU9UzIvKIiNwJICK3iMg8cA/wQRE50/m8iFyH0yL46/4Xf8jGJtgiduh1Jzt9cNMBXJnGmDBpJfNk22VqjZbnz6zXt8hpiVYquDPPeloBQ1WfBJ7c8drDXY+fxUnp7PbZ1zjazVv/EWEjNkl883A1gvUV5y5/Nh+8lWmMCRPJ5Jlefp7Fcp3rC97Wbl4s1clLmdJ4MKc/ABsZe2ibiWmSjVVUvd+P7kxtnJ60QG/MKMWzx5hinYU17728F1YrTFIhkQ3mqFiwQH9oreQUk5RZrXq/r9xZ1UYCOiGSMWGRmjxOXFoUl713qFhdvkhElHRApz8AC/SHJpkC06xzYa3m+TPb68zaoiPGjFRnCoPSIVaKK60sXPHZILJAf0jxiWNMS5nFUt3zZ6K1FZoSgzGb58aYUerMPlk5xEpxNXf6g1hAZ64EC/SHlpo8RlZqXFwre9q+2WqT3FqlFp8C2W3smTFmaNwu0luHmKq40RlJG+AWuQX6Q+rcUPXa9Fva2GSKMk2b/sCY0XODdesQY2F0w11QPB3c7pUW6A8p4naxqq54a/pdWHO6ZmmATxJjQsO9DiO1ouePROqde2zBvYYt0B+WWyPozGZ3kMVSnSnWiQa4D64xoZFIsxVJktxao7518KCpjc0mmWaJzeg4xBJDKOBgWKA/LLeLZNtj02+hVCMv69ur2xhjRqsxNs20lLlYPrhDRWew1FZAFwXvsEB/WJ3mW7XoadDUxdV1slK1eW6M8Yl2Kk+edS6sHRzoF0o1plhHAzz9AVigP7zUFIqQ1RJrHgZNVdyuWTZYyhh/iI7POF2kywePhVko1cnLOtEAd60EC/SHF4nSSEySp8yCh770tU43rgDfyDEmTBLZAtOy7un6XSzVmZYyYwFvkVugP4J2Os+0rHuqEWyvZmM1emN8ITZxjLysexr0uLDm3GOLjlugf92JZpwawUE5vlZbL3fjCvBgC2NCJZ0nxSbLK2sHbrq6ViROM/Atcgv0RxCfmGGag6dBWFrfZFLdEbRWozfGH9xrseZhkfDNzmpyAb9+LdAfgWQKFCIH5/gWSjWmZR1FIBXc1WmMCRW3dr7lYZHwrRBMfwAW6I8mnSfHBhdLlX03WyzVmaZMa2wSItEhFc4Ysy83aEdrK2w29x40VW00GWu4iwxZ6uZ1KFMgSpuNtf0HTV1w79hb10pjfMS9Hqcpc7G0uedmC6U607LufsYC/euPWyNolJf2HTS1WKpRiKxvz49jjPEBt3Y+LWUWSnv3nHNa5G6gt9TN65D71z3TXKNca+652UKpzrFIBQl4s8+YUEnm0EjM7SK99322BbdF3o4mIeFtfVm/skB/FN01gn360ncGWwT9jr0xoSKCpvJMs3+HikV3nipJTwd+LQlPgV5EbheRl0XkrIg8tMv7t4nIcyLSFJG7d7z3BhF5WkReEpEvish1/Sn6CLnNuGlZZ2GfvvSLa1XG2+uBb/YZEzaRTIHjsXUW9lkS9EKpzvHoRijusR0Y6EUkCjwK3AHcBNwnIjft2OwccD/w4V128QfAB1T1zcCtgPelXfxq+2bO3jWCVlupra8QpWU1emP8JpPnWLRyQI2+zrHoRiiuXy81+luBs6r6qqo2gMeBu7o3UNXXVPVFoN39uvsHIaaqz7jbbahqtT9FH6HYGJqYoCBlFve4mVPc2CSnJeeJ5eiN8Zd03pkGwUOOPgwtci+B/hrgfNfzefc1L74RWBORPxWRz4nIB9wWwhVE5AEROS0ip5eWDh7E4AeSnuZEYu8awYLbhx6wQG+M36QL5LR0YI4+2y6F4vr1Euh3uwtx8ETsjhjwTuAXgFuAG3BSPFfuTPUxVT2lqqdmZgIyeVCmsG/Tr7PgSGdbY4yPZAqkW+usbVRoNNtXvV1rtKhWK4y1a4HvQw/eAv08cG3X85PABY/7nwc+56Z9msCfA287XBF9Kl0gH9m7H+5Cqc6UhKMPrjGh49bSc1rZdaWpxbKzBKizbfCvXy+B/lngRhG5XkQSwL3AEx73/ywwJSKdavr3Al88fDF9KFMg13bmpN9t0NRiqc6xSOdECX6NwJhQ6eoivVuePmwt8gMDvVsTfxB4CngJ+JiqnhGRR0TkTgARuUVE5oF7gA+KyBn3sy2ctM0nROTzOGmg3x3MP2XI0tNkWmtUG03K9asHTV0o1Tk5VoXEOMSTIyigMWZPbvDOS5kLu3SxXFhzb8RCKCpqMS8bqeqTwJM7Xnu46/GzOCmd3T77DHBzD2X0p3SBWLtBmk0WS3VyqfgVby+WaszFNiAZ/JPEmNBJX+4ivdt046/H1I3ZTacv/R7zZSyU6hRC0gfXmNBxr8u5+O4dKhZKNU6OVa7YNsgs0B+V+1c+v8sCJO22crFcZ4pyKJp9xoSOuz7Etcnq7jX6Up1rx2ogEUhODrt0fecpdWN24QbwfGSdCztOlOXKJlstZaJVCkWzz5jQicYhOcmJWGXXFvmFtbqTeo1MQyT49eHg/wtGxe1b+8Zk7arRsU4NQUk2VkPRB9eYUMoUOBbZ2DV1s1gOV+rVAv1RuTX1N4xVrzpRFkp1UmwSbW9ajd4Yv0oXmJIySxubbCTwppcAAA4pSURBVLUuD5qqb7VYqTSY0nBMfwAW6I9ubAKiCU4krs7xLZbql/vgWo7eGH9K58m2S6jCpfXLK011BlBNtEuhaZFboD8qEUjnORa9egbLC6Uax6MbzpOQNP2MCZ1MntTWGsAV0xVfcKceT22thqaiZoG+F+kCU6yzsdlkvb61/fJiqc4Nmdr2NsYYH0oXSDRWAb2isrZYrhGhTWxzLTTXrwX6XmScph9wRfpmoVTnjcn69jbGGB/KFJB2kyzVq67fSTYQNDQtcgv0vUgXSDfdpl93jaBU52TCHWwRkqafMaHjXpsnd3SoWCzVeWOyesU2QWeBvhfpPInNFYDtvrjttrJYqnM8tgGROIxlR1lCY8xe3LTMN2TqV/Slv7BW5xvG3cBvgd6QKRBprJOQ5naNYKXaoNFqO71uMoXALypsTGi5adXr07WrcvSXU6+WujHuX/s3ZTa3c3yd35OEpw+uMaHkXp/X7OgivViqc81Y9Yptgs4CfS/cv/Y3jte3awSd35nmGqSnR1Y0Y8wB3IraXLzCpfU6zVabzWaL5Y0Gs7GNK7YJOgv0vXBPguuTte0cX+f3WGM1NM0+Y0IpkYZ4mpnIOm130NTFkjNwqiBl5/5aLDHiQvaHTWrWC7dZdzJZY2Hxco0+HhWitWJomn3GhFa6wKQ6C4wslJxaPUBO10NTmwcL9L3pmtN6vd5kY7PJYqnOiYkYUi9bjd4Yv8vkGW9dHgvTbDuBfry1Fqrr1wJ9L1JTgDDjrg27WKpxYa3GN040oI7l6I3xu3Se5MYy4KRdt1rO+s9jjVWYvHaUJesry9H3IhKF1BST6tQIFkp1Fst1bkh3+uCGp0ZgTCilC0RrRdKJqHP9lmpMJGNu6tVSN6YjU2Ci7eb41pzeN288Wd1+zxjjY5kCUi0ym0uyWKqz1Wozlx2DSjFU05dYoO9VuuAsMAJ8caFMo9nmRCJcfXCNCa10HraqXDcjLJRqNNvK9VmFciNU16+n1I2I3C4iL4vIWRF5aJf3bxOR50SkKSJ373ivJSLPuz9P9KvgvpGeJlIrUhhP8LlzTsA/FrW56I0JBPcavSGzyUKpzoW1Om9Kh2v6A/BQoxeRKPAo8P3APPCsiDyhql/s2uwccD/wC7vsoqaqb+lDWf0pU4Bzn2Y2l+TMBSeFMy3rgNjNWGP8zk2vvjFV5WI5TlvhDSGb/gC81ehvBc6q6quq2gAeB+7q3kBVX1PVF4H2bjsItXQBaivMTYzRbDt37LPtktMjJxIdceGMMfvangahgnv5hjL16iXQXwOc73o+777mVVJETovIp0Xkh3fbQEQecLc5vbS0dIhd+0CmANrmhgln4ZFYREhuhasPrjGh5V6ns7Hq9kuXV4cLT+rGS6DfbfpFPcQx3qCqp4AfB35bRN501c5UH1PVU6p6amZm5hC79gE3j9eZv/p4NkmkGq6uWcaElptezUt5+6XtxyG6hr0E+nmge+TASeCC1wOo6gX396vA/wbeeojy+V/X4gUAs7kkVJZDdZIYE1rJSYjEyLljYcBNvUbHIDE+woL1l5dA/yxwo4hcLyIJ4F7AU+8ZEZkSkTH3cQF4B/DF/T8VMG7T73jUWVFqNpeEatFSN8YEgQik84w11kjGI4yPxUg01kK3lsSBgV5Vm8CDwFPAS8DHVPWMiDwiIncCiMgtIjIP3AN8UETOuB9/M3BaRF4APgn8ux29dYLPvWFTcKdBOJFNOIE+RDdyjAm1tDNo6kQuxVxIW+SeBkyp6pPAkztee7jr8bM4KZ2dn/tb4Ft6LKO/uSdETku8YfobuXUuCtoK3YliTGilp6GyzC3XTROJCBRfp4He7COehMQ4sdoKf/Mv3wXLrzivW+rGmGDIFGDx8/z6T9/sPP/tZZi+YbRl6jOb1Kwf0nknXQOXf4esRmBMaKULl69bgOpK6FKvVqPvh0wBqs5Up1SWL79mjPG/TAFqq9BqOmnXxnqo+tCDBfr+SOdhfdF53An4VqM3Jhg612ptBVpbV74WEpa66Yfupl+nRh+ypp8xodUJ6pXlropauK5fq9H3Q8bN0as6+b3EuHOT1hjjf500a7UI7a0rXwsJC/T9kC5Asw6NilMjCFmzz5hQ69Teq8tOnr77tZCwQN8PncBeXQ7lYAtjQq07ddNuXvlaSFig74dOM69SdIL9+PHRlscY411n3Yhq0bkZKxFnmvEQsZux/ZDuyvGFsA+uMaEWjTuTm1WLzk9qGiLhCo1Wo++HzI7UTcj64BoTepmCm7rZCt2NWLBA3x+dfN7aeWjWQpffMyb00vnLN2NDeP2Gq30yKmNZiMRh6UvOc0vdGBMs6cLle2wW6M2uRJzm3vKXnechbPoZE2qdsTAhXUvCUjf9ku4K9FajNyZY0u58Ve1WKK9fC/T9kp6G1ublx8aY4EjnQ9uHHix10z/dzb0QNv2MCbWQX78W6Pul09yLxJ2bs8aY4OhO11iN3uypUwtI50O1qLAxrwvd6Var0Zs9dU6UEJ4kxoRexmr0xot0V43eGBMslroBEbldRF4WkbMi8tAu798mIs+JSFNE7t7l/ayIfF1E/lM/Cu1LnRqB1eiNCZ5EGuJp5/5abGzUpem7AwO9iESBR4E7gJuA+0Tkph2bnQPuBz68x25+FfjroxczAKxGb0ywpfOhvX691OhvBc6q6quq2gAeB+7q3kBVX1PVF4H2zg+LyLcBx4Gn+1Be/+qcICEcbGHM60KIA72XAVPXAOe7ns8D3+5l5yISAX4T+IfAu/fZ7gHgAYA3vOENXnbtP5kCvOtfwzf/yKhLYow5inf+C2cu+hDyEuh36yuoHvf/s8CTqnpe9ulyqKqPAY8BnDp1yuu+/UUEvvsXR10KY8xR3XTnqEswMF4C/Txwbdfzk8AFj/v/TuCdIvKzwDiQEJENVb3qhq4xxpjB8BLonwVuFJHrga8D9wI/7mXnqvq+zmMRuR84ZUHeGGOG68CElKo2gQeBp4CXgI+p6hkReURE7gQQkVtEZB64B/igiJwZZKGNMcZ4J6r+SomfOnVKT58+PepiGGNMoIjIZ1X11G7vhfMWszHGmG0W6I0xJuQs0BtjTMhZoDfGmJDz3c1YEVkCvtbDLgrAcp+KMwhWvt5Y+Xpj5euNn8v3RlWd2e0N3wX6XonI6b3uPPuBla83Vr7eWPl64/fy7cVSN8YYE3IW6I0xJuTCGOgfG3UBDmDl642VrzdWvt74vXy7Cl2O3hhjzJXCWKM3xhjTxQK9McaEXCADvYfFysdE5KPu+58RkeuGWLZrReSTIvKSiJwRkX+2yzbfIyIlEXne/Xl4WOXrKsNrIvJ59/hXzSInjt9xv8MXReRtQyzb3+v6bp4XkbKI/PyObYb6HYrIh0Tkkoh8oeu1aRF5RkRecX9P7fHZ97vbvCIi7x9i+T4gIl9y///+TEQm9/jsvufCAMv3KyLy9a7/wx/c47P7Xu8DLN9Hu8r2mog8v8dnB/799UxVA/UDRIGvADcACeAF4KYd2/ws8F/dx/cCHx1i+eaAt7mPJ4Av71K+7wH+csTf42tAYZ/3fxD4OM4KY98BfGaE/9+LOINBRvYdArcBbwO+0PXabwAPuY8fAn59l89NA6+6v6fcx1NDKt97gJj7+Nd3K5+Xc2GA5fsV4Bc8/P/ve70Pqnw73v9N4OFRfX+9/gSxRn/gYuXu8993H/8J8G7Zby3DPlLVBVV9zn28jjOH/zXDOHaf3QX8gTo+DUyKyNwIyvFu4Cuq2sto6Z6p6t8AKzte7j7Pfh/44V0++gPAM6q6oqqrwDPA7cMon6o+rc56EgCfxlkdbiT2+P688HK992y/8rmx40eBj/T7uMMSxEC/22LlOwPp9jbuiV4Chr68u5syeivwmV3e/k4ReUFEPi4i3zTUgjkUeFpEPusuzr6Tl+95GO5l7wts1N/hcVVdAOcPPHBsl2388j3+FE4LbTcHnQuD9KCbWvrQHqkvP3x/7wQuquore7w/yu/PkyAGei+LlfeyoHlfiMg48D+An1fV8o63n8NJRXwr8B+BPx9m2VzvUNW3AXcAPycit+143w/fYQK4E/jjXd72w3fohR++x18CmsAf7bHJQefCoPwX4E3AW4AFnPTITiP//oD72L82P6rvz7MgBnovi5VvbyMiMSDH0ZqNRyIicZwg/0eq+qc731fVsqpuuI+fBOIiUhhW+dzjXnB/XwL+DKeJ3K2XReH75Q7gOVW9uPMNP3yHwMVOOsv9fWmXbUb6Pbo3f38IeJ+6CeWdPJwLA6GqF1W1papt4Hf3OO6ov78Y8CPAR/faZlTf32EEMdBvL1bu1vjuBZ7Ysc0TQKd3w93AX+11kvebm8/778BLqvpbe2wz27lnICK34vw/FIdRPveYGRGZ6DzGuWn3hR2bPQH8I7f3zXcApU6aYoj2rEmN+jt0dZ9n7wf+YpdtngLeIyJTbmriPe5rAycitwP/CrhTVat7bOPlXBhU+brv+bx3j+N6ud4H6fuAL6nq/G5vjvL7O5RR3w0+yg9Oj5Av49yN/yX3tUdwTmiAJE5z/yzwd8ANQyzbd+E0LV8Ennd/fhD4GeBn3G0eBM7g9CD4NPD2IX9/N7jHfsEtR+c77C6jAI+63/HngVNDLmMaJ3Dnul4b2XeI8wdnAdjCqWX+NM59n08Ar7i/p91tTwH/reuzP+Wei2eBnxxi+c7i5Lc752GnJ9oJ4Mn9zoUhle8P3XPrRZzgPbezfO7zq673YZTPff33Oudc17ZD//56/bEpEIwxJuSCmLoxxhhzCBbojTEm5CzQG2NMyFmgN8aYkLNAb4wxIWeB3hhjQs4CvTHGhNz/B/i0Her1XFYjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 43.902998, Train accuracy: 0.199778, val accuracy: 0.209000\n",
      "Loss: 47.510867, Train accuracy: 0.191889, val accuracy: 0.186000\n",
      "Loss: 43.973404, Train accuracy: 0.201889, val accuracy: 0.209000\n",
      "Loss: 45.741678, Train accuracy: 0.202333, val accuracy: 0.206000\n",
      "Loss: 42.901077, Train accuracy: 0.220889, val accuracy: 0.241000\n",
      "Loss: 45.839594, Train accuracy: 0.211667, val accuracy: 0.211000\n",
      "Loss: 43.047903, Train accuracy: 0.229667, val accuracy: 0.232000\n",
      "Loss: 45.927764, Train accuracy: 0.214667, val accuracy: 0.220000\n",
      "Loss: 46.576661, Train accuracy: 0.201111, val accuracy: 0.208000\n",
      "Loss: 46.197443, Train accuracy: 0.214889, val accuracy: 0.208000\n",
      "Loss: 44.425546, Train accuracy: 0.205333, val accuracy: 0.212000\n",
      "Loss: 45.991710, Train accuracy: 0.219556, val accuracy: 0.220000\n",
      "Loss: 48.408707, Train accuracy: 0.215889, val accuracy: 0.212000\n",
      "Loss: 44.521148, Train accuracy: 0.200111, val accuracy: 0.211000\n",
      "Loss: 49.680944, Train accuracy: 0.227333, val accuracy: 0.223000\n",
      "Loss: 44.417620, Train accuracy: 0.212333, val accuracy: 0.217000\n",
      "Loss: 45.827925, Train accuracy: 0.207333, val accuracy: 0.203000\n",
      "Loss: 44.985096, Train accuracy: 0.207444, val accuracy: 0.224000\n",
      "Loss: 45.031737, Train accuracy: 0.208667, val accuracy: 0.216000\n",
      "Loss: 44.934009, Train accuracy: 0.224444, val accuracy: 0.224000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 44.765556, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 43.722158, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.589332, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 40.263818, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.279066, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 46.396806, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.855772, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 44.299011, Train accuracy: 0.196778, val accuracy: 0.209000\n",
      "Loss: 43.001508, Train accuracy: 0.203333, val accuracy: 0.211000\n",
      "Loss: 43.932037, Train accuracy: 0.206222, val accuracy: 0.211000\n",
      "Loss: 44.140161, Train accuracy: 0.211889, val accuracy: 0.209000\n",
      "Loss: 45.980239, Train accuracy: 0.212778, val accuracy: 0.215000\n",
      "Loss: 41.344856, Train accuracy: 0.217111, val accuracy: 0.221000\n",
      "Loss: 45.430768, Train accuracy: 0.224778, val accuracy: 0.223000\n",
      "Loss: 45.336049, Train accuracy: 0.227111, val accuracy: 0.216000\n",
      "Loss: 42.975567, Train accuracy: 0.225333, val accuracy: 0.224000\n",
      "Loss: 44.010070, Train accuracy: 0.214889, val accuracy: 0.220000\n",
      "Loss: 43.793564, Train accuracy: 0.228778, val accuracy: 0.225000\n",
      "Loss: 44.298501, Train accuracy: 0.227111, val accuracy: 0.215000\n",
      "Loss: 44.782337, Train accuracy: 0.236667, val accuracy: 0.223000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.548253, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 10.499019, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 13.149298, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: 1070.585658, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.333333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.266667\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.046581, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 12.374225, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 14.946815, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.266667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: nan, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: nan, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: nan, Train accuracy: 0.000000, val accuracy: 0.200000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
